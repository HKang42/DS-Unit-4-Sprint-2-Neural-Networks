diff --git a/.gitignore b/.gitignore
index 894a44c..9225091 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,3 +1,5 @@
+/logs
+
 # Byte-compiled / optimized / DLL files
 __pycache__/
 *.py[cod]
diff --git a/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb b/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
index e207e9d..a7ebb4d 100644
--- a/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
+++ b/module3-Intro-to-Keras/LS_DS_423_Keras_Assignment.ipynb
@@ -32,7 +32,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 8,
+   "execution_count": 1,
    "metadata": {
     "colab": {},
     "colab_type": "code",
@@ -49,7 +49,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 5,
+   "execution_count": 2,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -61,7 +61,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 19,
+   "execution_count": 3,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -72,7 +72,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 20,
+   "execution_count": 4,
    "metadata": {},
    "outputs": [
     {
@@ -82,7 +82,7 @@
        "        91.7    ,   3.9769 ,   4.     , 307.     ,  21.     ,  18.72   ])"
       ]
      },
-     "execution_count": 20,
+     "execution_count": 4,
      "metadata": {},
      "output_type": "execute_result"
     }
@@ -93,7 +93,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": 21,
+   "execution_count": 5,
    "metadata": {},
    "outputs": [
     {
@@ -102,7 +102,7 @@
        "(404, 12)"
       ]
      },
-     "execution_count": 21,
+     "execution_count": 5,
      "metadata": {},
      "output_type": "execute_result"
     }
diff --git a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
index 51e98d6..563a1f2 100644
--- a/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
+++ b/module4-Hyperparameter-Tuning/LS_DS_424_Hyperparameter_Tuning_Lecture.ipynb
@@ -42,12 +42,100 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 17,
    "metadata": {},
    "outputs": [],
    "source": [
-    "wandb_group = \"...\"\n",
-    "wandb_project = \"...\""
+    "wandb_group = \"ds8\"\n",
+    "wandb_project = \"ds13_inclass\"\n"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 6,
+   "metadata": {
+    "collapsed": true,
+    "jupyter": {
+     "outputs_hidden": true
+    }
+   },
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Collecting wandb\n",
+      "  Downloading wandb-0.8.36-py2.py3-none-any.whl (1.4 MB)\n",
+      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (2.22.0)\n",
+      "Collecting sentry-sdk>=0.4.0\n",
+      "  Downloading sentry_sdk-0.14.4-py2.py3-none-any.whl (104 kB)\n",
+      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (5.3)\n",
+      "Collecting configparser>=3.8.1\n",
+      "  Using cached configparser-5.0.0-py3-none-any.whl (22 kB)\n",
+      "Processing c:\\users\\hkang\\appdata\\local\\pip\\cache\\wheels\\b6\\9a\\56\\5456fd32264a8fc53eefcb2f74e24e99a7ef4eb40a9af5c905\\gql-0.2.0-py3-none-any.whl\n",
+      "Collecting shortuuid>=0.5.0\n",
+      "  Using cached shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
+      "Processing c:\\users\\hkang\\appdata\\local\\pip\\cache\\wheels\\df\\99\\da\\c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\\nvidia_ml_py3-7.352.0-py3-none-any.whl\n",
+      "Requirement already satisfied: six>=1.10.0 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (1.14.0)\n",
+      "Collecting docker-pycreds>=0.4.0\n",
+      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
+      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (2.8.1)\n",
+      "Requirement already satisfied: watchdog>=0.8.3 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (0.10.2)\n",
+      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (5.6.7)\n",
+      "Processing c:\\users\\hkang\\appdata\\local\\pip\\cache\\wheels\\50\\ca\\fa\\8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\\subprocess32-3.5.4-py3-none-any.whl\n",
+      "Requirement already satisfied: Click>=7.0 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from wandb) (7.0)\n",
+      "Collecting GitPython>=1.0.0\n",
+      "  Using cached GitPython-3.1.2-py3-none-any.whl (451 kB)\n",
+      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (1.25.8)\n",
+      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (2019.11.28)\n",
+      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (2.8)\n",
+      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from requests>=2.0.0->wandb) (3.0.4)\n",
+      "Processing c:\\users\\hkang\\appdata\\local\\pip\\cache\\wheels\\29\\93\\c6\\762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\\promise-2.3-py3-none-any.whl\n",
+      "Processing c:\\users\\hkang\\appdata\\local\\pip\\cache\\wheels\\6b\\fd\\8c\\a20dd591c1a554070cc33fb58042867e6ac1c85395abe2e57a\\graphql_core-1.1-py3-none-any.whl\n",
+      "Requirement already satisfied: pathtools>=0.1.1 in c:\\users\\hkang\\anaconda3\\lib\\site-packages (from watchdog>=0.8.3->wandb) (0.1.2)\n",
+      "Collecting gitdb<5,>=4.0.1\n",
+      "  Using cached gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
+      "Collecting smmap<4,>=3.0.1\n",
+      "  Using cached smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
+      "Installing collected packages: sentry-sdk, configparser, promise, graphql-core, gql, shortuuid, nvidia-ml-py3, docker-pycreds, subprocess32, smmap, gitdb, GitPython, wandb\n",
+      "Successfully installed GitPython-3.1.2 configparser-5.0.0 docker-pycreds-0.4.0 gitdb-4.0.5 gql-0.2.0 graphql-core-1.1 nvidia-ml-py3-7.352.0 promise-2.3 sentry-sdk-0.14.4 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.8.36\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "WARNING: You are using pip version 20.0.2; however, version 20.1 is available.\n",
+      "You should consider upgrading via the 'c:\\users\\hkang\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
+     ]
+    }
+   ],
+   "source": [
+    "# !pip install wandb"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 8,
+   "metadata": {},
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Successfully logged in to Weights & Biases!\n"
+     ]
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\hkang/.netrc\n"
+     ]
+    }
+   ],
+   "source": [
+    "! wandb login c89cc6fde6607ee05103a15f1daba325bfd50883"
    ]
   },
   {
@@ -81,7 +169,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 9,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -106,9 +194,46 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 10,
    "metadata": {},
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "[[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
+      "   0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
+      "   0.8252202 ]\n",
+      " [-0.40342651  2.99178419 -1.33391162 -0.25683275 -1.21518188  1.89434613\n",
+      "  -1.91036058  1.24758524 -0.85646254 -0.34843254 -1.71818909  0.43190599\n",
+      "  -1.32920239]\n",
+      " [ 0.1249402  -0.48361547  1.0283258  -0.25683275  0.62864202 -1.82968811\n",
+      "   1.11048828 -1.18743907  1.67588577  1.5652875   0.78447637  0.22061726\n",
+      "  -1.30850006]\n",
+      " [-0.40149354 -0.48361547 -0.86940196 -0.25683275 -0.3615597  -0.3245576\n",
+      "  -1.23667187  1.10717989 -0.51114231 -1.094663    0.78447637  0.44807713\n",
+      "  -0.65292624]\n",
+      " [-0.0056343  -0.48361547  1.0283258  -0.25683275  1.32861221  0.15364225\n",
+      "   0.69480801 -0.57857203  1.67588577  1.5652875   0.78447637  0.3898823\n",
+      "   0.26349695]\n",
+      " [-0.37502238 -0.48361547 -0.54747912 -0.25683275 -0.54935658 -0.78865126\n",
+      "   0.18954148  0.48371503 -0.51114231 -0.71552978  0.51145832  0.38669063\n",
+      "  -0.13812828]\n",
+      " [ 0.58963463 -0.48361547  1.0283258  -0.25683275  1.21764133 -1.03127774\n",
+      "   1.11048828 -1.06518235  1.67588577  1.5652875   0.78447637  0.44807713\n",
+      "   1.49873604]\n",
+      " [ 0.0381708  -0.48361547  1.24588095 -0.25683275  2.67733525 -1.12719983\n",
+      "   1.11048828 -1.14833073 -0.51114231 -0.01744323 -1.71818909  0.44807713\n",
+      "   1.88793986]\n",
+      " [-0.17228416 -0.48361547  1.24588095 -0.25683275  2.67733525 -0.90150078\n",
+      "   1.11048828 -1.09664657 -0.51114231 -0.01744323 -1.71818909 -1.97365769\n",
+      "   0.53952803]\n",
+      " [-0.22932104 -0.48361547  1.58544339 -0.25683275  0.56888847 -1.76056777\n",
+      "   1.11048828 -1.13471925 -0.62624905  0.18716835  1.23950646  0.44807713\n",
+      "   2.99068404]]\n"
+     ]
+    }
+   ],
    "source": [
     "from sklearn.preprocessing import StandardScaler\n",
     "\n",
@@ -133,7 +258,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 11,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -143,7 +268,174 @@
     "id": "GMXVfmzXp1Oo",
     "outputId": "b05e251e-508f-46e6-865b-f869ae2a5dc4"
    },
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 1/75\n",
+      "41/41 [==============================] - 0s 7ms/step - loss: 473.7040 - mse: 473.7040 - mae: 19.8271 - val_loss: 370.5274 - val_mse: 370.5274 - val_mae: 17.1988\n",
+      "Epoch 2/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 218.9933 - mse: 218.9933 - mae: 12.2209 - val_loss: 120.2020 - val_mse: 120.2020 - val_mae: 8.9611\n",
+      "Epoch 3/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 75.8348 - mse: 75.8348 - mae: 6.6269 - val_loss: 58.9606 - val_mse: 58.9606 - val_mae: 6.0607\n",
+      "Epoch 4/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 37.4351 - mse: 37.4351 - mae: 4.5599 - val_loss: 37.0682 - val_mse: 37.0682 - val_mae: 4.8104\n",
+      "Epoch 5/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 26.0413 - mse: 26.0413 - mae: 3.7046 - val_loss: 29.8580 - val_mse: 29.8580 - val_mae: 4.3478\n",
+      "Epoch 6/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 22.2569 - mse: 22.2569 - mae: 3.3225 - val_loss: 27.8733 - val_mse: 27.8733 - val_mae: 4.1532\n",
+      "Epoch 7/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 19.7973 - mse: 19.7973 - mae: 3.1329 - val_loss: 25.0773 - val_mse: 25.0773 - val_mae: 3.8363\n",
+      "Epoch 8/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 18.2668 - mse: 18.2668 - mae: 3.0331 - val_loss: 25.0450 - val_mse: 25.0450 - val_mae: 3.7061\n",
+      "Epoch 9/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 16.8397 - mse: 16.8397 - mae: 2.8821 - val_loss: 24.4529 - val_mse: 24.4529 - val_mae: 3.6707\n",
+      "Epoch 10/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 15.4907 - mse: 15.4907 - mae: 2.7786 - val_loss: 22.7019 - val_mse: 22.7019 - val_mae: 3.5099\n",
+      "Epoch 11/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 14.5501 - mse: 14.5501 - mae: 2.6517 - val_loss: 23.1885 - val_mse: 23.1885 - val_mae: 3.4824\n",
+      "Epoch 12/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 13.7494 - mse: 13.7494 - mae: 2.5960 - val_loss: 23.8931 - val_mse: 23.8931 - val_mae: 3.5172\n",
+      "Epoch 13/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 13.3959 - mse: 13.3959 - mae: 2.5946 - val_loss: 21.9510 - val_mse: 21.9510 - val_mae: 3.3233\n",
+      "Epoch 14/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 12.4131 - mse: 12.4131 - mae: 2.5134 - val_loss: 23.3329 - val_mse: 23.3329 - val_mae: 3.3424\n",
+      "Epoch 15/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 11.9994 - mse: 11.9994 - mae: 2.4618 - val_loss: 22.6189 - val_mse: 22.6189 - val_mae: 3.2713\n",
+      "Epoch 16/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 11.3630 - mse: 11.3630 - mae: 2.3737 - val_loss: 23.0445 - val_mse: 23.0445 - val_mae: 3.2998\n",
+      "Epoch 17/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 11.1456 - mse: 11.1456 - mae: 2.3601 - val_loss: 22.4451 - val_mse: 22.4451 - val_mae: 3.2274\n",
+      "Epoch 18/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 10.8282 - mse: 10.8282 - mae: 2.3372 - val_loss: 22.6533 - val_mse: 22.6533 - val_mae: 3.2028\n",
+      "Epoch 19/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 10.5128 - mse: 10.5128 - mae: 2.2888 - val_loss: 24.1406 - val_mse: 24.1406 - val_mae: 3.2333\n",
+      "Epoch 20/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 10.2183 - mse: 10.2183 - mae: 2.2596 - val_loss: 22.7212 - val_mse: 22.7212 - val_mae: 3.1393\n",
+      "Epoch 21/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 10.1550 - mse: 10.1550 - mae: 2.2852 - val_loss: 21.7652 - val_mse: 21.7652 - val_mae: 3.1064\n",
+      "Epoch 22/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 9.9509 - mse: 9.9509 - mae: 2.2160 - val_loss: 22.1521 - val_mse: 22.1521 - val_mae: 3.0777\n",
+      "Epoch 23/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 9.5895 - mse: 9.5895 - mae: 2.1865 - val_loss: 22.1525 - val_mse: 22.1525 - val_mae: 3.0462\n",
+      "Epoch 24/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 9.4708 - mse: 9.4708 - mae: 2.1856 - val_loss: 21.6640 - val_mse: 21.6640 - val_mae: 3.0476\n",
+      "Epoch 25/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 9.4106 - mse: 9.4106 - mae: 2.1644 - val_loss: 21.7976 - val_mse: 21.7976 - val_mae: 2.9929\n",
+      "Epoch 26/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 9.2728 - mse: 9.2728 - mae: 2.1590 - val_loss: 21.7188 - val_mse: 21.7188 - val_mae: 3.0012\n",
+      "Epoch 27/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 9.0184 - mse: 9.0184 - mae: 2.1216 - val_loss: 22.1830 - val_mse: 22.1830 - val_mae: 3.0850\n",
+      "Epoch 28/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 9.0959 - mse: 9.0959 - mae: 2.1363 - val_loss: 22.4786 - val_mse: 22.4786 - val_mae: 3.0233\n",
+      "Epoch 29/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 8.8110 - mse: 8.8110 - mae: 2.1150 - val_loss: 21.4887 - val_mse: 21.4887 - val_mae: 2.9711\n",
+      "Epoch 30/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.7245 - mse: 8.7245 - mae: 2.1127 - val_loss: 21.2224 - val_mse: 21.2224 - val_mae: 2.9318\n",
+      "Epoch 31/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.7508 - mse: 8.7508 - mae: 2.0999 - val_loss: 22.0238 - val_mse: 22.0238 - val_mae: 2.9863\n",
+      "Epoch 32/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.7495 - mse: 8.7495 - mae: 2.1070 - val_loss: 23.2741 - val_mse: 23.2741 - val_mae: 3.0812\n",
+      "Epoch 33/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 8.4718 - mse: 8.4718 - mae: 2.0814 - val_loss: 20.8961 - val_mse: 20.8961 - val_mae: 2.9170\n",
+      "Epoch 34/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.4596 - mse: 8.4596 - mae: 2.0738 - val_loss: 20.4318 - val_mse: 20.4318 - val_mae: 2.8852\n",
+      "Epoch 35/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.1297 - mse: 8.1297 - mae: 2.0263 - val_loss: 21.1989 - val_mse: 21.1989 - val_mae: 2.9452\n",
+      "Epoch 36/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.2409 - mse: 8.2409 - mae: 2.0246 - val_loss: 21.8364 - val_mse: 21.8364 - val_mae: 2.9513\n",
+      "Epoch 37/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 8.1167 - mse: 8.1167 - mae: 2.0468 - val_loss: 21.2305 - val_mse: 21.2305 - val_mae: 2.9148\n",
+      "Epoch 38/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.8906 - mse: 7.8906 - mae: 1.9857 - val_loss: 20.4233 - val_mse: 20.4233 - val_mae: 2.8679\n",
+      "Epoch 39/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 8.2212 - mse: 8.2212 - mae: 2.0640 - val_loss: 20.8262 - val_mse: 20.8262 - val_mae: 2.9808\n",
+      "Epoch 40/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 7.6627 - mse: 7.6627 - mae: 1.9430 - val_loss: 20.7697 - val_mse: 20.7697 - val_mae: 2.9155\n",
+      "Epoch 41/75\n",
+      "41/41 [==============================] - 0s 5ms/step - loss: 7.7675 - mse: 7.7675 - mae: 1.9857 - val_loss: 19.3587 - val_mse: 19.3587 - val_mae: 2.7998\n",
+      "Epoch 42/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.7884 - mse: 7.7884 - mae: 1.9993 - val_loss: 20.5551 - val_mse: 20.5551 - val_mae: 2.9041\n",
+      "Epoch 43/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.4663 - mse: 7.4663 - mae: 1.9244 - val_loss: 20.0553 - val_mse: 20.0553 - val_mae: 2.8249\n",
+      "Epoch 44/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.5322 - mse: 7.5322 - mae: 1.9501 - val_loss: 20.2704 - val_mse: 20.2704 - val_mae: 2.8890\n",
+      "Epoch 45/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.5140 - mse: 7.5140 - mae: 1.9380 - val_loss: 22.3724 - val_mse: 22.3724 - val_mae: 3.0755\n",
+      "Epoch 46/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.3412 - mse: 7.3412 - mae: 1.9078 - val_loss: 19.6411 - val_mse: 19.6411 - val_mae: 2.8573\n",
+      "Epoch 47/75\n",
+      "41/41 [==============================] - 0s 5ms/step - loss: 7.1458 - mse: 7.1458 - mae: 1.9086 - val_loss: 18.5300 - val_mse: 18.5300 - val_mae: 2.8347\n",
+      "Epoch 48/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 7.2324 - mse: 7.2324 - mae: 1.9004 - val_loss: 18.7207 - val_mse: 18.7207 - val_mae: 2.8298\n",
+      "Epoch 49/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.9843 - mse: 6.9843 - mae: 1.8822 - val_loss: 19.5137 - val_mse: 19.5137 - val_mae: 2.8097\n",
+      "Epoch 50/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.8246 - mse: 6.8246 - mae: 1.8778 - val_loss: 18.1907 - val_mse: 18.1907 - val_mae: 2.7490\n",
+      "Epoch 51/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 6.9673 - mse: 6.9673 - mae: 1.8709 - val_loss: 18.8116 - val_mse: 18.8116 - val_mae: 2.8121\n",
+      "Epoch 52/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.8162 - mse: 6.8162 - mae: 1.8407 - val_loss: 18.0881 - val_mse: 18.0881 - val_mae: 2.7275\n",
+      "Epoch 53/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.5808 - mse: 6.5808 - mae: 1.8293 - val_loss: 20.0118 - val_mse: 20.0118 - val_mae: 2.9749\n",
+      "Epoch 54/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 6.9457 - mse: 6.9457 - mae: 1.9347 - val_loss: 20.4536 - val_mse: 20.4536 - val_mae: 2.9512\n",
+      "Epoch 55/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.5081 - mse: 6.5081 - mae: 1.8405 - val_loss: 17.8034 - val_mse: 17.8034 - val_mae: 2.7028\n",
+      "Epoch 56/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.4875 - mse: 6.4875 - mae: 1.8048 - val_loss: 17.4836 - val_mse: 17.4836 - val_mae: 2.6791\n",
+      "Epoch 57/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 6.4320 - mse: 6.4320 - mae: 1.7994 - val_loss: 17.2207 - val_mse: 17.2207 - val_mae: 2.6734\n",
+      "Epoch 58/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.1886 - mse: 6.1886 - mae: 1.7688 - val_loss: 18.3401 - val_mse: 18.3401 - val_mae: 2.7485\n",
+      "Epoch 59/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.1788 - mse: 6.1788 - mae: 1.7396 - val_loss: 16.5648 - val_mse: 16.5648 - val_mae: 2.6320\n",
+      "Epoch 60/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 6.0243 - mse: 6.0243 - mae: 1.7515 - val_loss: 17.7263 - val_mse: 17.7263 - val_mae: 2.7620\n",
+      "Epoch 61/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 6.0309 - mse: 6.0309 - mae: 1.7460 - val_loss: 15.9253 - val_mse: 15.9253 - val_mae: 2.6070\n",
+      "Epoch 62/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 6.1429 - mse: 6.1429 - mae: 1.7546 - val_loss: 17.1352 - val_mse: 17.1352 - val_mae: 2.6776\n",
+      "Epoch 63/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 6.1149 - mse: 6.1149 - mae: 1.7488 - val_loss: 17.4956 - val_mse: 17.4956 - val_mae: 2.7187\n",
+      "Epoch 64/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 5.8176 - mse: 5.8176 - mae: 1.7087 - val_loss: 18.0398 - val_mse: 18.0398 - val_mae: 2.7763\n",
+      "Epoch 65/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 5.8803 - mse: 5.8803 - mae: 1.7176 - val_loss: 17.6793 - val_mse: 17.6793 - val_mae: 2.7847\n",
+      "Epoch 66/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 5.7964 - mse: 5.7964 - mae: 1.7378 - val_loss: 16.7580 - val_mse: 16.7580 - val_mae: 2.7064\n",
+      "Epoch 67/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 5.6401 - mse: 5.6401 - mae: 1.6673 - val_loss: 15.8765 - val_mse: 15.8765 - val_mae: 2.5906\n",
+      "Epoch 68/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 5.4978 - mse: 5.4978 - mae: 1.6522 - val_loss: 15.7602 - val_mse: 15.7602 - val_mae: 2.6123\n",
+      "Epoch 69/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 5.4346 - mse: 5.4346 - mae: 1.6464 - val_loss: 16.3351 - val_mse: 16.3351 - val_mae: 2.6340\n",
+      "Epoch 70/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 5.4164 - mse: 5.4164 - mae: 1.6633 - val_loss: 15.5962 - val_mse: 15.5962 - val_mae: 2.6026\n",
+      "Epoch 71/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 5.2871 - mse: 5.2871 - mae: 1.6217 - val_loss: 15.8760 - val_mse: 15.8760 - val_mae: 2.6352\n",
+      "Epoch 72/75\n",
+      "41/41 [==============================] - 0s 3ms/step - loss: 5.1733 - mse: 5.1733 - mae: 1.6000 - val_loss: 15.8016 - val_mse: 15.8016 - val_mae: 2.6469\n",
+      "Epoch 73/75\n",
+      "41/41 [==============================] - 0s 2ms/step - loss: 5.1856 - mse: 5.1856 - mae: 1.6063 - val_loss: 14.7692 - val_mse: 14.7692 - val_mae: 2.5924\n",
+      "Epoch 74/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 5.1948 - mse: 5.1948 - mae: 1.6292 - val_loss: 15.9395 - val_mse: 15.9395 - val_mae: 2.6872\n",
+      "Epoch 75/75\n",
+      "41/41 [==============================] - 0s 4ms/step - loss: 5.0891 - mse: 5.0891 - mae: 1.6132 - val_loss: 14.3026 - val_mse: 14.3026 - val_mae: 2.5196\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x25558704e80>"
+      ]
+     },
+     "execution_count": 11,
+     "metadata": {},
+     "output_type": "execute_result"
+    }
+   ],
    "source": [
     "from tensorflow import keras\n",
     "from tensorflow.keras.models import Sequential\n",
@@ -165,8 +457,11 @@
     "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
     "\n",
     "# Fit Model\n",
+    "# We'll just specify the test set as the validation. A different option would \n",
+    "# be to just define a percent of the training data\n",
     "model.fit(x_train, y_train, \n",
     "          validation_data=(x_test,y_test), \n",
+    "          #validation_split = 0.2\n",
     "          epochs=epochs, \n",
     "          batch_size=batch_size\n",
     "         )"
@@ -255,7 +550,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 12,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -265,7 +560,20 @@
     "id": "2smXfriNAGn7",
     "outputId": "ae996575-78e2-43fb-9dbe-5d44aaf0b430"
    },
-   "outputs": [],
+   "outputs": [
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Best: 0.6484424114227295 using {'batch_size': 8, 'epochs': 20}\n",
+      "Means: 0.6484424114227295, Stdev: 0.03502330921593287 with: {'batch_size': 8, 'epochs': 20}\n",
+      "Means: 0.6380358219146729, Stdev: 0.012303059024725022 with: {'batch_size': 16, 'epochs': 20}\n",
+      "Means: 0.6093031287193298, Stdev: 0.04161140444724694 with: {'batch_size': 32, 'epochs': 20}\n",
+      "Means: 0.5221203744411469, Stdev: 0.03947348474709791 with: {'batch_size': 64, 'epochs': 20}\n",
+      "Means: 0.5431457579135894, Stdev: 0.0767844791656975 with: {'batch_size': 128, 'epochs': 20}\n"
+     ]
+    }
+   ],
    "source": [
     "import numpy\n",
     "import pandas as pd\n",
@@ -305,7 +613,7 @@
     "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
     "\n",
     "# define the grid search parameters\n",
-    "param_grid = {'batch_size': [10, 20, 40, 60, 80, 100],\n",
+    "param_grid = {'batch_size': [8, 16, 32, 64, 128],\n",
     "              'epochs': [20]}\n",
     "\n",
     "# Create Grid Search\n",
@@ -466,7 +774,7 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 13,
    "metadata": {},
    "outputs": [],
    "source": [
@@ -476,7 +784,30 @@
   },
   {
    "cell_type": "code",
-   "execution_count": null,
+   "execution_count": 25,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "wandb_group = \"ds8\"\n",
+    "wandb_project = \"ds13_inclass\""
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 27,
+   "metadata": {},
+   "outputs": [],
+   "source": [
+    "from tensorflow.keras.optimizers import Adam\n",
+    "\n",
+    "lr = 0.01\n",
+    "\n",
+    "Opt_function = Adam(learning_rate = lr)"
+   ]
+  },
+  {
+   "cell_type": "code",
+   "execution_count": 28,
    "metadata": {
     "colab": {
      "base_uri": "https://localhost:8080/",
@@ -486,17 +817,226 @@
     "id": "GMXVfmzXp1Oo",
     "outputId": "b05e251e-508f-46e6-865b-f869ae2a5dc4"
    },
-   "outputs": [],
+   "outputs": [
+    {
+     "data": {
+      "text/html": [
+       "\n",
+       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
+       "                Project page: <a href=\"https://app.wandb.ai/ds8/ds13_inclass\" target=\"_blank\">https://app.wandb.ai/ds8/ds13_inclass</a><br/>\n",
+       "                Run page: <a href=\"https://app.wandb.ai/ds8/ds13_inclass/runs/2v3oogyk\" target=\"_blank\">https://app.wandb.ai/ds8/ds13_inclass/runs/2v3oogyk</a><br/>\n",
+       "            "
+      ],
+      "text/plain": [
+       "<IPython.core.display.HTML object>"
+      ]
+     },
+     "metadata": {},
+     "output_type": "display_data"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
+      "wandb: Wandb version 0.8.36 is available!  To upgrade, please run:\n",
+      "wandb:  $ pip install wandb --upgrade\n"
+     ]
+    },
+    {
+     "name": "stdout",
+     "output_type": "stream",
+     "text": [
+      "Epoch 1/80\n",
+      "17/17 [==============================] - 0s 12ms/step - loss: 197.1880 - mse: 197.1880 - mae: 11.0648 - val_loss: 138.9335 - val_mse: 138.9335 - val_mae: 9.5157\n",
+      "Epoch 2/80\n",
+      "17/17 [==============================] - 0s 9ms/step - loss: 48.7138 - mse: 48.7138 - mae: 5.4167 - val_loss: 38.5116 - val_mse: 38.5116 - val_mae: 4.5819\n",
+      "Epoch 3/80\n",
+      "17/17 [==============================] - 0s 7ms/step - loss: 17.2343 - mse: 17.2343 - mae: 3.0733 - val_loss: 22.9562 - val_mse: 22.9562 - val_mae: 3.1735\n",
+      "Epoch 4/80\n",
+      "17/17 [==============================] - 0s 7ms/step - loss: 12.5356 - mse: 12.5356 - mae: 2.7556 - val_loss: 20.6751 - val_mse: 20.6751 - val_mae: 2.9720\n",
+      "Epoch 5/80\n",
+      "17/17 [==============================] - 0s 7ms/step - loss: 10.0411 - mse: 10.0411 - mae: 2.4341 - val_loss: 17.1182 - val_mse: 17.1182 - val_mae: 2.7359\n",
+      "Epoch 6/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 9.7151 - mse: 9.7151 - mae: 2.3456 - val_loss: 17.7193 - val_mse: 17.7193 - val_mae: 2.9033\n",
+      "Epoch 7/80\n",
+      "17/17 [==============================] - 0s 8ms/step - loss: 9.2447 - mse: 9.2447 - mae: 2.3327 - val_loss: 15.5484 - val_mse: 15.5484 - val_mae: 2.6971\n",
+      "Epoch 8/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 9.0259 - mse: 9.0259 - mae: 2.2950 - val_loss: 17.3300 - val_mse: 17.3300 - val_mae: 2.8488\n",
+      "Epoch 9/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 8.9447 - mse: 8.9447 - mae: 2.2751 - val_loss: 16.4510 - val_mse: 16.4510 - val_mae: 2.6898\n",
+      "Epoch 10/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 7.6805 - mse: 7.6805 - mae: 2.0763 - val_loss: 15.9760 - val_mse: 15.9760 - val_mae: 2.6704\n",
+      "Epoch 11/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 6.9613 - mse: 6.9613 - mae: 1.9746 - val_loss: 16.6721 - val_mse: 16.6721 - val_mae: 2.7492\n",
+      "Epoch 12/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 6.7505 - mse: 6.7505 - mae: 1.9081 - val_loss: 16.1996 - val_mse: 16.1996 - val_mae: 2.7574\n",
+      "Epoch 13/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 6.8496 - mse: 6.8496 - mae: 1.9529 - val_loss: 17.3338 - val_mse: 17.3338 - val_mae: 2.8470\n",
+      "Epoch 14/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 6.7819 - mse: 6.7819 - mae: 1.9593 - val_loss: 16.0633 - val_mse: 16.0633 - val_mae: 2.6895\n",
+      "Epoch 15/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 7.2693 - mse: 7.2693 - mae: 2.0502 - val_loss: 16.1089 - val_mse: 16.1089 - val_mae: 2.7316\n",
+      "Epoch 16/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 8.0378 - mse: 8.0378 - mae: 2.2666 - val_loss: 16.1731 - val_mse: 16.1731 - val_mae: 2.7054\n",
+      "Epoch 17/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 7.3184 - mse: 7.3184 - mae: 2.0222 - val_loss: 16.3586 - val_mse: 16.3586 - val_mae: 2.7903\n",
+      "Epoch 18/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 6.0005 - mse: 6.0005 - mae: 1.8486 - val_loss: 15.7143 - val_mse: 15.7143 - val_mae: 2.7102\n",
+      "Epoch 19/80\n",
+      "17/17 [==============================] - 0s 7ms/step - loss: 4.8219 - mse: 4.8219 - mae: 1.6121 - val_loss: 15.2135 - val_mse: 15.2135 - val_mae: 2.5700\n",
+      "Epoch 20/80\n",
+      "17/17 [==============================] - 0s 8ms/step - loss: 4.8217 - mse: 4.8217 - mae: 1.7020 - val_loss: 14.8773 - val_mse: 14.8773 - val_mae: 2.4652\n",
+      "Epoch 21/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 5.1412 - mse: 5.1412 - mae: 1.7227 - val_loss: 15.0517 - val_mse: 15.0517 - val_mae: 2.6178\n",
+      "Epoch 22/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 5.8769 - mse: 5.8769 - mae: 1.8622 - val_loss: 15.0082 - val_mse: 15.0082 - val_mae: 2.5948\n",
+      "Epoch 23/80\n",
+      "17/17 [==============================] - 0s 8ms/step - loss: 6.3808 - mse: 6.3808 - mae: 1.8723 - val_loss: 16.7329 - val_mse: 16.7329 - val_mae: 2.8096\n",
+      "Epoch 24/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 5.6298 - mse: 5.6298 - mae: 1.8102 - val_loss: 16.8546 - val_mse: 16.8546 - val_mae: 2.9557\n",
+      "Epoch 25/80\n",
+      "17/17 [==============================] - 0s 8ms/step - loss: 8.1747 - mse: 8.1747 - mae: 2.1277 - val_loss: 14.6313 - val_mse: 14.6313 - val_mae: 2.8729\n",
+      "Epoch 26/80\n",
+      "17/17 [==============================] - 0s 9ms/step - loss: 7.5343 - mse: 7.5343 - mae: 2.1956 - val_loss: 14.9819 - val_mse: 14.9819 - val_mae: 2.6236\n",
+      "Epoch 27/80\n",
+      "17/17 [==============================] - 0s 12ms/step - loss: 5.6095 - mse: 5.6095 - mae: 1.6798 - val_loss: 13.6813 - val_mse: 13.6813 - val_mae: 2.4738\n",
+      "Epoch 28/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.5697 - mse: 4.5697 - mae: 1.6231 - val_loss: 14.8403 - val_mse: 14.8403 - val_mae: 2.6038\n",
+      "Epoch 29/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.0407 - mse: 4.0407 - mae: 1.5087 - val_loss: 13.7706 - val_mse: 13.7706 - val_mae: 2.4375\n",
+      "Epoch 30/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 4.4551 - mse: 4.4551 - mae: 1.6255 - val_loss: 15.2189 - val_mse: 15.2189 - val_mae: 2.5778\n",
+      "Epoch 31/80\n",
+      "17/17 [==============================] - 0s 8ms/step - loss: 5.0342 - mse: 5.0342 - mae: 1.6947 - val_loss: 13.4125 - val_mse: 13.4125 - val_mae: 2.4795\n",
+      "Epoch 32/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.3108 - mse: 4.3108 - mae: 1.6687 - val_loss: 14.0293 - val_mse: 14.0293 - val_mae: 2.5092\n",
+      "Epoch 33/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.1663 - mse: 4.1663 - mae: 1.5413 - val_loss: 14.3122 - val_mse: 14.3122 - val_mae: 2.5584\n",
+      "Epoch 34/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.8765 - mse: 3.8765 - mae: 1.5100 - val_loss: 13.8588 - val_mse: 13.8588 - val_mae: 2.5591\n",
+      "Epoch 35/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.6050 - mse: 3.6050 - mae: 1.4566 - val_loss: 14.9036 - val_mse: 14.9036 - val_mae: 2.3764\n",
+      "Epoch 36/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.3145 - mse: 3.3145 - mae: 1.3745 - val_loss: 17.7012 - val_mse: 17.7012 - val_mae: 2.8372\n",
+      "Epoch 37/80\n",
+      "17/17 [==============================] - 0s 7ms/step - loss: 4.3872 - mse: 4.3872 - mae: 1.5835 - val_loss: 15.7159 - val_mse: 15.7159 - val_mae: 2.6613\n",
+      "Epoch 38/80\n",
+      "17/17 [==============================] - 0s 11ms/step - loss: 6.1369 - mse: 6.1369 - mae: 1.8904 - val_loss: 13.3110 - val_mse: 13.3110 - val_mae: 2.4003\n",
+      "Epoch 39/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 4.7163 - mse: 4.7163 - mae: 1.6993 - val_loss: 20.2242 - val_mse: 20.2242 - val_mae: 3.0424\n",
+      "Epoch 40/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 4.8383 - mse: 4.8383 - mae: 1.7349 - val_loss: 13.4678 - val_mse: 13.4678 - val_mae: 2.5288\n",
+      "Epoch 41/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 4.4687 - mse: 4.4687 - mae: 1.6210 - val_loss: 16.4704 - val_mse: 16.4704 - val_mae: 2.7293\n",
+      "Epoch 42/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.2338 - mse: 3.2338 - mae: 1.3127 - val_loss: 14.3879 - val_mse: 14.3879 - val_mae: 2.4510\n",
+      "Epoch 43/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.0680 - mse: 3.0680 - mae: 1.3516 - val_loss: 13.9952 - val_mse: 13.9952 - val_mae: 2.5056\n",
+      "Epoch 44/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.4165 - mse: 3.4165 - mae: 1.3832 - val_loss: 15.5739 - val_mse: 15.5739 - val_mae: 2.4574\n",
+      "Epoch 45/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.3330 - mse: 3.3330 - mae: 1.3842 - val_loss: 14.3030 - val_mse: 14.3030 - val_mae: 2.4564\n",
+      "Epoch 46/80\n",
+      "17/17 [==============================] - 0s 7ms/step - loss: 3.3410 - mse: 3.3410 - mae: 1.3907 - val_loss: 13.8006 - val_mse: 13.8006 - val_mae: 2.5675\n",
+      "Epoch 47/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 5.0212 - mse: 5.0212 - mae: 1.6550 - val_loss: 15.3674 - val_mse: 15.3674 - val_mae: 2.6718\n",
+      "Epoch 48/80\n",
+      "17/17 [==============================] - 0s 11ms/step - loss: 3.4136 - mse: 3.4136 - mae: 1.3997 - val_loss: 12.8511 - val_mse: 12.8511 - val_mae: 2.4109\n",
+      "Epoch 49/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 3.1289 - mse: 3.1289 - mae: 1.3153 - val_loss: 14.8851 - val_mse: 14.8851 - val_mae: 2.4864\n",
+      "Epoch 50/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.9097 - mse: 3.9097 - mae: 1.4869 - val_loss: 14.5656 - val_mse: 14.5656 - val_mae: 2.6810\n",
+      "Epoch 51/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.9978 - mse: 3.9978 - mae: 1.4595 - val_loss: 16.9582 - val_mse: 16.9582 - val_mae: 2.5645\n",
+      "Epoch 52/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 5.2240 - mse: 5.2240 - mae: 1.7669 - val_loss: 17.4779 - val_mse: 17.4779 - val_mae: 2.8841\n",
+      "Epoch 53/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.2304 - mse: 4.2304 - mae: 1.5844 - val_loss: 16.8578 - val_mse: 16.8578 - val_mae: 2.9354\n",
+      "Epoch 54/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 3.6075 - mse: 3.6075 - mae: 1.3697 - val_loss: 17.0641 - val_mse: 17.0641 - val_mae: 2.6863\n",
+      "Epoch 55/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 3.8208 - mse: 3.8208 - mae: 1.4153 - val_loss: 15.8249 - val_mse: 15.8249 - val_mae: 2.8642\n",
+      "Epoch 56/80\n",
+      "17/17 [==============================] - 0s 9ms/step - loss: 3.4407 - mse: 3.4407 - mae: 1.4429 - val_loss: 16.3282 - val_mse: 16.3282 - val_mae: 2.6261\n",
+      "Epoch 57/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.9155 - mse: 3.9155 - mae: 1.5239 - val_loss: 13.9373 - val_mse: 13.9373 - val_mae: 2.5315\n",
+      "Epoch 58/80\n",
+      "17/17 [==============================] - 0s 6ms/step - loss: 4.7591 - mse: 4.7591 - mae: 1.6661 - val_loss: 17.4088 - val_mse: 17.4088 - val_mae: 2.9860\n",
+      "Epoch 59/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 3.1133 - mse: 3.1133 - mae: 1.3579 - val_loss: 14.0425 - val_mse: 14.0425 - val_mae: 2.5043\n",
+      "Epoch 60/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.6477 - mse: 3.6477 - mae: 1.4191 - val_loss: 13.9998 - val_mse: 13.9998 - val_mae: 2.5893\n",
+      "Epoch 61/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.8566 - mse: 2.8566 - mae: 1.3025 - val_loss: 13.5976 - val_mse: 13.5976 - val_mae: 2.4631\n",
+      "Epoch 62/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 2.5627 - mse: 2.5627 - mae: 1.2204 - val_loss: 15.2463 - val_mse: 15.2463 - val_mae: 2.6865\n",
+      "Epoch 63/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.7311 - mse: 2.7311 - mae: 1.2670 - val_loss: 13.5114 - val_mse: 13.5114 - val_mae: 2.4422\n",
+      "Epoch 64/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.7569 - mse: 2.7569 - mae: 1.2532 - val_loss: 13.9408 - val_mse: 13.9408 - val_mae: 2.3553\n",
+      "Epoch 65/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.2075 - mse: 4.2075 - mae: 1.5328 - val_loss: 16.2309 - val_mse: 16.2309 - val_mae: 3.1820\n",
+      "Epoch 66/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.4205 - mse: 3.4205 - mae: 1.4424 - val_loss: 13.7590 - val_mse: 13.7590 - val_mae: 2.5949\n",
+      "Epoch 67/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.5085 - mse: 3.5085 - mae: 1.4240 - val_loss: 13.2990 - val_mse: 13.2990 - val_mae: 2.3552\n",
+      "Epoch 68/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.6852 - mse: 3.6852 - mae: 1.4655 - val_loss: 15.7473 - val_mse: 15.7473 - val_mae: 2.6955\n",
+      "Epoch 69/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 4.5781 - mse: 4.5781 - mae: 1.6446 - val_loss: 15.7905 - val_mse: 15.7905 - val_mae: 2.7944\n",
+      "Epoch 70/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.4774 - mse: 3.4774 - mae: 1.4345 - val_loss: 15.3425 - val_mse: 15.3425 - val_mae: 2.5784\n",
+      "Epoch 71/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.1608 - mse: 3.1608 - mae: 1.3241 - val_loss: 14.6083 - val_mse: 14.6083 - val_mae: 2.6059\n",
+      "Epoch 72/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.6422 - mse: 3.6422 - mae: 1.5078 - val_loss: 14.3006 - val_mse: 14.3006 - val_mae: 2.6319\n",
+      "Epoch 73/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.9177 - mse: 2.9177 - mae: 1.3538 - val_loss: 13.3565 - val_mse: 13.3565 - val_mae: 2.5115\n",
+      "Epoch 74/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.3639 - mse: 2.3639 - mae: 1.1329 - val_loss: 14.4827 - val_mse: 14.4827 - val_mae: 2.5341\n",
+      "Epoch 75/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.0890 - mse: 3.0890 - mae: 1.3046 - val_loss: 14.1094 - val_mse: 14.1094 - val_mae: 2.4424\n",
+      "Epoch 76/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.8966 - mse: 2.8966 - mae: 1.2969 - val_loss: 13.5906 - val_mse: 13.5906 - val_mae: 2.4470\n",
+      "Epoch 77/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 2.6199 - mse: 2.6199 - mae: 1.2487 - val_loss: 14.8716 - val_mse: 14.8716 - val_mae: 2.6641\n",
+      "Epoch 78/80\n",
+      "17/17 [==============================] - 0s 5ms/step - loss: 3.3200 - mse: 3.3200 - mae: 1.3662 - val_loss: 16.2850 - val_mse: 16.2850 - val_mae: 2.6148\n",
+      "Epoch 79/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.0038 - mse: 3.0038 - mae: 1.2982 - val_loss: 17.7273 - val_mse: 17.7273 - val_mae: 2.8966\n",
+      "Epoch 80/80\n",
+      "17/17 [==============================] - 0s 4ms/step - loss: 3.3028 - mse: 3.3028 - mae: 1.3900 - val_loss: 13.1950 - val_mse: 13.1950 - val_mae: 2.3419\n"
+     ]
+    },
+    {
+     "data": {
+      "text/plain": [
+       "<tensorflow.python.keras.callbacks.History at 0x25560952eb8>"
+      ]
+     },
+     "execution_count": 28,
+     "metadata": {},
+     "output_type": "execute_result"
+    },
+    {
+     "name": "stderr",
+     "output_type": "stream",
+     "text": [
+      "Run pip install nbformat to save notebook history\n"
+     ]
+    }
+   ],
    "source": [
-    "wandb.init(project=wandb_project, entity=wand_group) #Initializes and Experiment\n",
+    "wandb.init(project=wandb_project, entity=wandb_group) #Initializes and Experiment\n",
     "\n",
     "# Important Hyperparameters\n",
     "X =  x_train\n",
     "y =  y_train\n",
     "\n",
     "inputs = X.shape[1]\n",
-    "wandb.config.epochs = 50\n",
-    "wandb.config.batch_size = 10\n",
+    "wandb.config.epochs = 80\n",
+    "wandb.config.batch_size = 16\n",
     "\n",
     "# Create Model\n",
     "model = Sequential()\n",
@@ -505,7 +1045,7 @@
     "model.add(Dense(64, activation='relu'))\n",
     "model.add(Dense(1))\n",
     "# Compile Model\n",
-    "model.compile(optimizer='adam', loss='mse', metrics=['mse', 'mae'])\n",
+    "model.compile(optimizer=Opt_function, loss='mse', metrics=['mse', 'mae'])\n",
     "\n",
     "# Fit Model\n",
     "model.fit(X, y, \n",
@@ -724,9 +1264,9 @@
  ],
  "metadata": {
   "kernelspec": {
-   "display_name": "U4-S2-NNF-DS12",
+   "display_name": "U4-S2-Neural-Networks (Python3)",
    "language": "python",
-   "name": "u4-s2-nnf-ds12"
+   "name": "u4-s2-neural-networks"
   },
   "language_info": {
    "codemirror_mode": {
@@ -738,7 +1278,7 @@
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
-   "version": "3.7.7"
+   "version": "3.7.0"
   }
  },
  "nbformat": 4,
